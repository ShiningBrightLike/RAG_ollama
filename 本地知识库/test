1.深度交叉网络DCN
"""
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding, Concatenate
from tensorflow.keras.models import Model

class CrossNetwork(Layer):
    """交叉网络核心实现"""
    def __init__(self, num_layers):
        super().__init__()
        self.num_layers = num_layers
    
    def build(self, input_shape):
        dim = input_shape[-1]
        # 修改变量名以避免与保留属性冲突
        self.cross_weights = [self.add_weight(
            name=f'cross_weight_{i}',
            shape=(dim, 1), 
            initializer='random_normal', 
            trainable=True) 
            for i in range(self.num_layers)]
        self.cross_biases = [self.add_weight(
            name=f'cross_bias_{i}',
            shape=(dim, 1), 
            initializer='random_normal', 
            trainable=True)
            for i in range(self.num_layers)]
    
    def call(self, inputs):
        x_0 = tf.expand_dims(inputs, axis=2)  # (batch, dim, 1)
        x_l = x_0
        
        for i in range(self.num_layers):
            # x_l^T * w (batch, 1, 1)
            xl_w = tf.matmul(tf.transpose(x_l, [0, 2, 1]), self.cross_weights[i])
            # x_0 * (x_l^T * w) (batch, dim, 1)
            xl_w = tf.matmul(x_0, xl_w)
            # 加上偏置和原始x_l
            x_l = xl_w + self.cross_biases[i] + x_l
        
        return tf.squeeze(x_l, axis=2)  # (batch, dim)

class DCN(Model):
    """深度交叉网络完整模型"""
    def __init__(self, feature_dims, embed_dim=8, num_cross_layers=3, hidden_units=[64, 32]):
        """
        参数:
            feature_dims: 各特征域的维度，如[100, 50, 200]表示3个特征域，每个域的取值空间大小
            embed_dim: 嵌入维度
            num_cross_layers: 交叉层数
            hidden_units: 深度网络隐藏层单元数
        """
        super().__init__()
        # 嵌入层
        self.embed_layers = [Embedding(dim, embed_dim) for dim in feature_dims]
        
        # 交叉网络
        self.cross_net = CrossNetwork(num_cross_layers)
        
        # 深度网络
        self.deep_net = tf.keras.Sequential([
            Dense(units, activation='relu') for units in hidden_units
        ])
        
        # 输出层
        self.output_layer = Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        # 输入形状: (batch, num_features)
        # 特征嵌入
        embed_outputs = []
        for i in range(len(self.embed_layers)):
            embed = self.embed_layers[i](inputs[:, i])  # (batch, embed_dim)
            embed_outputs.append(embed)
        
        # 拼接所有特征
        x = tf.concat(embed_outputs, axis=1)  # (batch, num_features*embed_dim)
        
        # 交叉网络
        cross_output = self.cross_net(x)
        
        # 深度网络
        deep_output = self.deep_net(x)
        
        # 拼接两部分输出
        concat_output = tf.concat([cross_output, deep_output], axis=1)
        
        # 最终输出
        return self.output_layer(concat_output)

# 使用示例
if __name__ == "__main__":
    # 假设有3个特征域，取值空间分别为100, 50, 200
    feature_dims = [100, 50, 200]
    
    # 创建模型
    model = DCN(feature_dims, embed_dim=8, num_cross_layers=3, hidden_units=[64, 32, 16])
    
    # 构建模型
    model.build(input_shape=(None, len(feature_dims)))
    model.summary()
    
    # 测试前向传播
    test_input = tf.constant([[10, 20, 30], [40, 10, 50]])  # 2个样本，3个特征
    output = model(test_input)
    print("Output shape:", output)
"""

2.多头注意力机制MHA
"""
import tensorflow as tf

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        
        assert d_model % self.num_heads == 0
        self.depth = d_model // self.num_heads
        
        # 初始化四个全连接层
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
    
    def split_heads(self, x, batch_size):
        # 分割多头 [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, depth]
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def scaled_dot_product_attention(self, q, k, v, mask=None):
        # 计算注意力权重 [batch_size, num_heads, seq_len_q, seq_len_k]
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(tf.cast(self.depth, tf.float32))
        
        # 添加mask
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
        
        # softmax归一化
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)
        return output, attention_weights
    
    def call(self, q, k, v, mask=None):
        batch_size = tf.shape(q)[0]
        
        # 线性变换 + 分割多头
        q = self.wq(q)  # [batch_size, seq_len_q, d_model]
        k = self.wk(k)  # [batch_size, seq_len_k, d_model]
        v = self.wv(v)  # [batch_size, seq_len_v, d_model]
        
        q = self.split_heads(q, batch_size)  # [batch_size, num_heads, seq_len_q, depth]
        k = self.split_heads(k, batch_size)  # [batch_size, num_heads, seq_len_k, depth]
        v = self.split_heads(v, batch_size)  # [batch_size, num_heads, seq_len_v, depth]
        
        # 计算缩放点积注意力
        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)
        # [batch_size, num_heads, seq_len_q, depth]
        
        # 拼接多头 [batch_size, seq_len_q, d_model]
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        
        # 最终全连接层
        output = self.dense(concat_attention)
        return output, attention_weights

# 示例使用
if __name__ == "__main__":
    mha = MultiHeadAttention(d_model=512, num_heads=8)
    q = tf.random.uniform((32, 64, 512))  # [batch_size, seq_len, d_model]
    k = v = tf.random.uniform((32, 64, 512))
    output, attn = mha(q, k, v)
    print("Output shape:", output.shape)  # (32, 64, 512)

"""

