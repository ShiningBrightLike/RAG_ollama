1.深度交叉网络DCN
"""
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding

class CrossNetwork(Layer):
    """精简版交叉网络（核心逻辑不变）"""
    def __init__(self, num_layers):
        super().__init__()
        self.num_layers = num_layers
    
    def build(self, input_shape):
        dim = input_shape[-1]
        # 合并权重和偏置初始化
        self.weights_ = [self.add_weight(
            shape=(dim, 1), 
            initializer='glorot_normal') 
            for _ in range(self.num_layers)]
        self.biases = [self.add_weight(
            shape=(dim, 1),
            initializer='zeros')
            for _ in range(self.num_layers)]
    
    def call(self, x):
        x_0 = tf.expand_dims(x, 2)  # (batch, dim, 1)
        x_l = x_0
        
        for w, b in zip(self.weights_, self.biases):
            # 核心交叉公式: x_l = x0 * (x_l^T * w) + b + x_l
            x_l = tf.matmul(x_0, tf.matmul(tf.transpose(x_l, [0,2,1]), w)) + b + x_l
        
        return tf.squeeze(x_l, 2)  # (batch, dim)


class DCN(tf.keras.Model):
    """精简版深度交叉网络"""
    def __init__(self, feature_dims, embed_dim=8, num_cross=2):
        super().__init__()
        # 1. 嵌入层
        self.embeds = [Embedding(d, embed_dim) for d in feature_dims]
        
        # 2. 交叉网络
        self.cross = CrossNetwork(num_cross)

        # 3. 深度网络
        self.dnn = [Dense(embed_dim, activation='relu') for _ in range(2)]
        
        # 4. 输出层 (合并交叉网络和原始特征)
        self.out = Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        # 特征嵌入 (batch, num_features) -> list of (batch, embed_dim)
        x = tf.concat([emb(inputs[:,i]) for i, emb in enumerate(self.embeds)], axis=1)
        
        # 交叉网络
        cross = self.cross(x)

        # 深度网络
        for layer in self.dnn:
            x =  layer(x)
        
        # 合并原始特征和交叉特征
        return self.out(tf.concat([cross, x], axis=1))

# 测试用例
if __name__ == "__main__":
    # 模拟数据: 3个特征域，每个域取值空间为[100, 50, 200]
    model = DCN([100, 50, 200], embed_dim=8)
    
    test_input = tf.constant([[10, 20, 30], [40, 10, 50]])  # 2个样本
    print("Input shape:", test_input.shape)  # (2, 3)
    print("Output shape:", model(test_input).shape)  # (2, 1)
        
"""

2.多头注意力机制MHA
"""
import tensorflow as tf

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        
        assert d_model % self.num_heads == 0
        self.depth = d_model // self.num_heads
        
        # 初始化四个全连接层
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
    
    def split_heads(self, x, batch_size):
        # 分割多头 [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, depth]
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def scaled_dot_product_attention(self, q, k, v, mask=None):
        # 计算注意力权重 [batch_size, num_heads, seq_len_q, seq_len_k]
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(tf.cast(self.depth, tf.float32))
        
        # 添加mask
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
        
        # softmax归一化
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)
        return output, attention_weights
    
    def call(self, q, k, v, mask=None):
        batch_size = tf.shape(q)[0]
        
        # 线性变换 + 分割多头
        q = self.wq(q)  # [batch_size, seq_len_q, d_model]
        k = self.wk(k)  # [batch_size, seq_len_k, d_model]
        v = self.wv(v)  # [batch_size, seq_len_v, d_model]
        
        q = self.split_heads(q, batch_size)  # [batch_size, num_heads, seq_len_q, depth]
        k = self.split_heads(k, batch_size)  # [batch_size, num_heads, seq_len_k, depth]
        v = self.split_heads(v, batch_size)  # [batch_size, num_heads, seq_len_v, depth]
        
        # 计算缩放点积注意力
        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)
        # [batch_size, num_heads, seq_len_q, depth]
        
        # 拼接多头 [batch_size, seq_len_q, d_model]
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        
        # 最终全连接层
        output = self.dense(concat_attention)
        return output, attention_weights

# 示例使用
if __name__ == "__main__":
    mha = MultiHeadAttention(d_model=512, num_heads=8)
    q = tf.random.uniform((32, 64, 512))  # [batch_size, seq_len, d_model]
    k = v = tf.random.uniform((32, 64, 512))
    output, attn = mha(q, k, v)
    print("Output shape:", output.shape)  # (32, 64, 512)

"""

3.深度兴趣网络DIN
"""
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding
from tensorflow.keras.models import Model

class DINAttention(Layer):
    """精简版DIN注意力层"""
    def __init__(self, units=16, activation='relu'):
        super().__init__()
        self.dense1 = Dense(units, activation=activation)
        self.dense2 = Dense(1)  # 注意力分数输出

    def call(self, query, keys):  # 明确参数名称
        # 1. 对齐维度
        query = tf.expand_dims(query, 1)  # (batch,1,emb_dim)
        query = tf.tile(query, [1, tf.shape(keys)[1], 1])  # (batch,seq_len,emb_dim)
        
        # 2. 特征交互计算
        att_input = tf.concat([query, keys, query * keys], axis=-1)
        
        # 3. 计算注意力权重
        att_weights = tf.nn.softmax(
            self.dense2(self.dense1(att_input)),  # (batch,seq_len,1)
            axis=1
        )
        
        # 4. 加权求和
        return tf.reduce_sum(att_weights * keys, axis=1)  # (batch,emb_dim)

class DIN(Model):
    """精简版DIN模型"""
    def __init__(self, emb_dim=8):
        super().__init__()
        self.item_embed = Embedding(100, emb_dim)
        self.seq_embed = Embedding(100, emb_dim)
        self.attention = DINAttention()
        self.dense = Dense(1, activation='sigmoid')

    def call(self, inputs):
        # 明确解构输入字典
        item_ids = inputs['item']
        seq_ids = inputs['hist']
        
        item_emb = tf.squeeze(self.item_embed(item_ids), axis=1)  # (batch,emb_dim)
        seq_emb = self.seq_embed(seq_ids)  # (batch,seq_len,emb_dim)
        
        # 使用命名参数调用
        user_emb = self.attention(query=item_emb, keys=seq_emb)
        
        # 拼接特征并预测
        concat = tf.concat([user_emb, item_emb], axis=-1)
        return self.dense(concat)

# 测试运行
if __name__ == "__main__":
    # 1. 实例化模型
    model = DIN()
    
    # 2. 生成模拟数据（确保数据类型一致）
    mock_data = {
        'item': tf.constant([[1], [2]], dtype=tf.int32),
        'hist': tf.constant([[1,2,3], [4,5,6]], dtype=tf.int32)
    }
    
    # 3. 前向传播验证
    print("输入形状：")
    print({k: v.shape for k, v in mock_data.items()})
    print("输出形状：", model(mock_data).shape)

"""

