1.深度交叉网络DCN
"""
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding

class CrossNetwork(Layer):
    """精简版交叉网络（核心逻辑不变）"""
    def __init__(self, num_layers):
        super().__init__()
        self.num_layers = num_layers
    
    def build(self, input_shape):
        dim = input_shape[-1]
        # 合并权重和偏置初始化
        self.weights_ = [self.add_weight(
            shape=(dim, 1), 
            initializer='glorot_normal') 
            for _ in range(self.num_layers)]
        self.biases = [self.add_weight(
            shape=(dim, 1),
            initializer='zeros')
            for _ in range(self.num_layers)]
    
    def call(self, x):
        x_0 = tf.expand_dims(x, 2)  # (batch, dim, 1)
        x_l = x_0
        
        for w, b in zip(self.weights_, self.biases):
            # 核心交叉公式: x_l = x0 * (x_l^T * w) + b + x_l
            x_l = tf.matmul(x_0, tf.matmul(tf.transpose(x_l, [0,2,1]), w)) + b + x_l
        
        return tf.squeeze(x_l, 2)  # (batch, dim)


class DCN(tf.keras.Model):
    """精简版深度交叉网络"""
    def __init__(self, feature_dims, embed_dim=8, num_cross=2):
        super().__init__()
        # 1. 嵌入层
        self.embeds = [Embedding(d, embed_dim) for d in feature_dims]
        
        # 2. 交叉网络
        self.cross = CrossNetwork(num_cross)

        # 3. 深度网络
        self.dnn = [Dense(embed_dim, activation='relu') for _ in range(2)]
        
        # 4. 输出层 (合并交叉网络和原始特征)
        self.out = Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        # 特征嵌入 (batch, num_features) -> list of (batch, embed_dim)
        x = tf.concat([emb(inputs[:,i]) for i, emb in enumerate(self.embeds)], axis=1)
        
        # 交叉网络
        cross = self.cross(x)

        # 深度网络
        for layer in self.dnn:
            x =  layer(x)
        
        # 合并原始特征和交叉特征
        return self.out(tf.concat([cross, x], axis=1))

# 测试用例
if __name__ == "__main__":
    # 模拟数据: 3个特征域，每个域取值空间为[100, 50, 200]
    model = DCN([100, 50, 200], embed_dim=8)
    
    test_input = tf.constant([[10, 20, 30], [40, 10, 50]])  # 2个样本
    print("Input shape:", test_input.shape)  # (2, 3)
    print("Output shape:", model(test_input).shape)  # (2, 1)
        
"""

2.多头注意力机制MHA
"""
import tensorflow as tf

class MultiHeadAttention(tf.keras.layers.Layer):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.d_model = d_model
        
        assert d_model % self.num_heads == 0
        self.depth = d_model // self.num_heads
        
        # 初始化四个全连接层
        self.wq = tf.keras.layers.Dense(d_model)
        self.wk = tf.keras.layers.Dense(d_model)
        self.wv = tf.keras.layers.Dense(d_model)
        self.dense = tf.keras.layers.Dense(d_model)
    
    def split_heads(self, x, batch_size):
        # 分割多头 [batch_size, seq_len, d_model] -> [batch_size, num_heads, seq_len, depth]
        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    def scaled_dot_product_attention(self, q, k, v, mask=None):
        # 计算注意力权重 [batch_size, num_heads, seq_len_q, seq_len_k]
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(tf.cast(self.depth, tf.float32))
        
        # 添加mask
        if mask is not None:
            scaled_attention_logits += (mask * -1e9)
        
        # softmax归一化
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)
        return output, attention_weights
    
    def call(self, q, k, v, mask=None):
        batch_size = tf.shape(q)[0]
        
        # 线性变换 + 分割多头
        q = self.wq(q)  # [batch_size, seq_len_q, d_model]
        k = self.wk(k)  # [batch_size, seq_len_k, d_model]
        v = self.wv(v)  # [batch_size, seq_len_v, d_model]
        
        q = self.split_heads(q, batch_size)  # [batch_size, num_heads, seq_len_q, depth]
        k = self.split_heads(k, batch_size)  # [batch_size, num_heads, seq_len_k, depth]
        v = self.split_heads(v, batch_size)  # [batch_size, num_heads, seq_len_v, depth]
        
        # 计算缩放点积注意力
        scaled_attention, attention_weights = self.scaled_dot_product_attention(q, k, v, mask)
        # [batch_size, num_heads, seq_len_q, depth]
        
        # 拼接多头 [batch_size, seq_len_q, d_model]
        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])
        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))
        
        # 最终全连接层
        output = self.dense(concat_attention)
        return output, attention_weights

# 示例使用
if __name__ == "__main__":
    mha = MultiHeadAttention(d_model=512, num_heads=8)
    q = tf.random.uniform((32, 64, 512))  # [batch_size, seq_len, d_model]
    k = v = tf.random.uniform((32, 64, 512))
    output, attn = mha(q, k, v)
    print("Output shape:", output.shape)  # (32, 64, 512)

"""

3.深度兴趣网络DIN
"""
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense, Embedding
from tensorflow.keras.models import Model

class DINAttention(Layer):
    """精简版DIN注意力层"""
    def __init__(self, units=16, activation='relu'):
        super().__init__()
        self.dense1 = Dense(units, activation=activation)
        self.dense2 = Dense(1)  # 注意力分数输出

    def call(self, query, keys):  # 明确参数名称
        # 1. 对齐维度
        query = tf.expand_dims(query, 1)  # (batch,1,emb_dim)
        query = tf.tile(query, [1, tf.shape(keys)[1], 1])  # (batch,seq_len,emb_dim)
        
        # 2. 特征交互计算
        att_input = tf.concat([query, keys, query * keys], axis=-1)
        
        # 3. 计算注意力权重
        att_weights = tf.nn.softmax(
            self.dense2(self.dense1(att_input)),  # (batch,seq_len,1)
            axis=1
        )
        
        # 4. 加权求和
        return tf.reduce_sum(att_weights * keys, axis=1)  # (batch,emb_dim)

class DIN(Model):
    """精简版DIN模型"""
    def __init__(self, emb_dim=8):
        super().__init__()
        self.item_embed = Embedding(100, emb_dim)
        self.seq_embed = Embedding(100, emb_dim)
        self.attention = DINAttention()
        self.dense = Dense(1, activation='sigmoid')

    def call(self, inputs):
        # 明确解构输入字典
        item_ids = inputs['item']
        seq_ids = inputs['hist']
        
        item_emb = tf.squeeze(self.item_embed(item_ids), axis=1)  # (batch,emb_dim)
        seq_emb = self.seq_embed(seq_ids)  # (batch,seq_len,emb_dim)
        
        # 使用命名参数调用
        user_emb = self.attention(query=item_emb, keys=seq_emb)
        
        # 拼接特征并预测
        concat = tf.concat([user_emb, item_emb], axis=-1)
        return self.dense(concat)

# 测试运行
if __name__ == "__main__":
    # 1. 实例化模型
    model = DIN()
    
    # 2. 生成模拟数据（确保数据类型一致）
    mock_data = {
        'item': tf.constant([[1], [2]], dtype=tf.int32),
        'hist': tf.constant([[1,2,3], [4,5,6]], dtype=tf.int32)
    }
    
    # 3. 前向传播验证
    print("输入形状：")
    print({k: v.shape for k, v in mock_data.items()})
    print("输出形状：", model(mock_data).shape)

"""

4. AUC drop
"""
import numpy as np
from sklearn.utils import shuffle
from sklearn.metrics import roc_auc_score
from MMoE_model import MMoE, build_mmoe_model
import tensorflow as tf
import pandas as pd
def calculate_feature_importance(model, X_categorical, X_numeric, y_list, categorical_cols, numeric_cols, n_iter):
    """
    计算每个特征对各个任务的重要性
    
    参数:
        model: 训练好的MMoE模型
        X_categorical: 类别特征列表
        X_numeric: 数值特征数组
        y_list: 各任务标签列表
        categorical_cols: 类别特征列名
        numeric_cols: 数值特征列名
        n_iter: 每个特征打乱重复次数
        
    返回:
        包含各特征对各任务AUC影响的字典
    """
    # 计算基准AUC
    baseline_auc = []
    for i in range(4):
        y_pred = model.predict(X_categorical + [X_numeric])[i]
        baseline_auc.append(roc_auc_score(y_list[i], y_pred))
    
    # 初始化结果存储
    feature_importance = {
        'feature': [],
        'task1_auc_drop': [],
        'task2_auc_drop': [],
        'task3_auc_drop': [],
        'task4_auc_drop': []
    }
    
    # 处理类别特征
    for i, col in enumerate(categorical_cols):
        print(f"Processing categorical feature: {col}")
        auc_drops = np.zeros(4)
        
        for _ in range(n_iter):
            # 打乱当前特征
            shuffled_X_cat = X_categorical.copy()
            shuffled_X_cat[i] = shuffle(shuffled_X_cat[i])
            
            # 预测并计算AUC
            y_preds = model.predict(shuffled_X_cat + [X_numeric])
            for j in range(4):
                auc = roc_auc_score(y_list[j], y_preds[j])
                auc_drops[j] += (baseline_auc[j] - auc) / n_iter
        
        # 记录结果
        feature_importance['feature'].append(col)
        for j in range(4):
            feature_importance[f'task{j+1}_auc_drop'].append(auc_drops[j])
    
    # 处理数值特征
    for i, col in enumerate(numeric_cols):
        print(f"Processing numeric feature: {col}")
        auc_drops = np.zeros(4)
        
        for _ in range(n_iter):
            # 打乱当前特征
            shuffled_X_num = X_numeric.copy()
            shuffled_X_num[:, i] = shuffle(shuffled_X_num[:, i])
            
            # 预测并计算AUC
            y_preds = model.predict(X_categorical + [shuffled_X_num])
            for j in range(4):
                auc = roc_auc_score(y_list[j], y_preds[j])
                auc_drops[j] += (baseline_auc[j] - auc) / n_iter
        
        # 记录结果
        feature_importance['feature'].append(col)
        for j in range(4):
            feature_importance[f'task{j+1}_auc_drop'].append(auc_drops[j])
    
    return pd.DataFrame(feature_importance)

# 使用示例
if __name__ == '__main__':
    # 定义特征列（必须与训练时完全一致）
    categorical_cols = [
        'date', 'hourmin', 'tab', 
        'user_active_degree', 'is_lowactive_period', 'is_live_streamer','is_video_author',
        'onehot_feat0', 'onehot_feat1', 'onehot_feat2', 'onehot_feat3', 'onehot_feat4',
        'onehot_feat5', 'onehot_feat6', 'onehot_feat7', 'onehot_feat8', 'onehot_feat9',
        'onehot_feat10', 'onehot_feat11', 'onehot_feat12', 'onehot_feat13', 'onehot_feat14',
        'onehot_feat15', 'onehot_feat16', 'onehot_feat17',
        'follow_user_num_range', 'fans_user_num_range', 'friend_user_num_range', 'register_days_range', 
        'video_type', 'upload_dt', 'upload_type', 'visible_status','music_type', 'tag'
    ]

    numeric_cols = [
        'follow_user_num', 'fans_user_num', 'friend_user_num', 'register_days',
        'video_duration', 'server_width', 'server_height',
        'counts', 'show_cnt', 'show_user_num', 'play_cnt',
        'play_user_num', 'play_duration', 'complete_play_cnt',
        'complete_play_user_num', 'valid_play_cnt', 'valid_play_user_num',
        'long_time_play_cnt', 'long_time_play_user_num', 'short_time_play_cnt',
        'short_time_play_user_num', 'play_progress', 'comment_stay_duration',
        'like_cnt', 'like_user_num', 'click_like_cnt', 'double_click_cnt',
        'cancel_like_cnt', 'cancel_like_user_num', 'comment_cnt',
        'comment_user_num', 'direct_comment_cnt', 'reply_comment_cnt',
        'delete_comment_cnt', 'delete_comment_user_num', 'comment_like_cnt',
        'comment_like_user_num', 'follow_cnt', 'follow_user_num1',
        'cancel_follow_cnt', 'cancel_follow_user_num', 'share_cnt',
        'share_user_num', 'download_cnt', 'download_user_num', 'report_cnt',
        'report_user_num', 'reduce_similar_cnt', 'reduce_similar_user_num',
        'collect_cnt', 'collect_user_num', 'cancel_collect_cnt',
        'cancel_collect_user_num', 'direct_comment_user_num',
        'reply_comment_user_num', 'share_all_cnt', 'share_all_user_num',
        'outsite_share_all_cnt'
    ]

    # 加载模型（需要指定自定义对象）
    model = tf.keras.models.load_model(
        "KuaiRand-Pure\saved\mmoe_model_20250703220102.h5",
        custom_objects={'MMoE': MMoE}
    )

    # 验证模型加载成功
    model.summary()
    
    # 准备标签数据
    df_X_test = pd.read_parquet("KuaiRand-Pure/data_processed/processed_X_test.parquet")
    df_y_test = pd.read_parquet("KuaiRand-Pure/data_processed/processed_y_test.parquet")
    X_test_categorical = [df_X_test[col].astype('int32').values for col in categorical_cols]
    X_test_numeric = df_X_test[numeric_cols].astype('float32').values
    y_test_task1 = df_y_test.iloc[:, 0].values.reshape(-1, 1).astype(np.float32)
    y_test_task2 = df_y_test.iloc[:, 1].values.reshape(-1, 1).astype(np.float32)
    y_test_task3 = df_y_test.iloc[:, 2].values.reshape(-1, 1).astype(np.float32)
    y_test_task4 = df_y_test.iloc[:, 3].values.reshape(-1, 1).astype(np.float32)
    y_list = [y_test_task1, y_test_task2, y_test_task3, y_test_task4]

    # 计算特征重要性
    importance_df = calculate_feature_importance(
        model=model,
        X_categorical=X_test_categorical,
        X_numeric=X_test_numeric,
        y_list=y_list,
        categorical_cols=categorical_cols,
        numeric_cols=numeric_cols,
        n_iter=1  # 每个特征打乱3次取平均
    )
    
    # 保存结果
    importance_df.to_csv("feature_importance_results.csv", index=False)
    
    # 打印最重要的特征
    for i in range(4):
        print(f"\nTop 10 features for Task {i+1}:")
        print(importance_df.sort_values(by=f'task{i+1}_auc_drop', ascending=False)[['feature', f'task{i+1}_auc_drop']].head(10))
"""
